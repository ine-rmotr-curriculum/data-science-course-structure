{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Saving Arrays to HDF5\n",
    "\n",
    "The Hierarchical Data Format was created by the National Center for Supercomputing Applications (NCSA) in 1987. It consists of *datasets*, which are multidimensional arrays of a single data type, and *groups*, which are container structures holding datasets and other groups.  Both groups and datasets may have attributes attached to them, which are any pieces of named metadata.  What this, in effect, does is to emulate a filesystem within a single file.  The nodes or \"files\" within this virtual filesystem are array objects.  Generally, a single HDF5 file will contain a variety of related data for working with the same underlying problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Network Common Data Form (NetCDF) is a library of functions for storing and retrieving array data.  The project itself is nearly as old as HDF, and is an open standard that was developed and supported by a variety of scientific agencies. As of its version 4, it supports using HDF5 as its storage backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Generic HDF5 files typically have an extension of `.h5`, `.hdf5`, `.hdf`, or `.he5`.  These should all represent the same binary format, and other extensions occur sometimes too. Some corresponding extensions for HDF4 also exist.  Even though NetCDF can consist of numerous underlying file formats, they all seem standardized on the `.nc` or `.nc4` extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lazy Data Access\n",
    "\n",
    "HDF5 is often used to store large datasets, up to gigabytes or terabytes in size.  Therefore, accessing a dataset is performed internally as a memory map to a relevant disk region rather than an eager read of the data into memory.  Operations like NumPy slices into an array only concretize into RAM the specific values needed for the action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Within Python, there are two popular open source libraries for working with HDF5, **PyTables** and **h5py**. PyTables and h5py have moderately different attitudes.  H5py stays close to HDF5 spec itself while PyTables attempts to provide a higher-level \"Pythonic\" interface that was, like `lxml.objectify`, inspired by my defunct `gnosis.xml.objectify` library from 2000.  In this lesson, we use `h5py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For illustration, let us look at a 42 MiB NetCDF/HDF5 file published by NASA that contains satellite measurements.\n",
    "The names for data files used by NASA are verbose but contain detailed indication in the names themselves of the nature of the data sets in them.  Displayed is the data set name, its dimensions, its data type, its shape, and the 'units' attribute that these all have.  In general, attributes may have any names, but NASA has conventions about which to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELP   | (1494, 60, 47) | float32 | Pa\n",
      "PBLTOP | (1494, 60)     | float32 | Pa\n",
      "PHIS   | (1494, 60)     | float32 | m+2 s-2\n",
      "PS     | (1494, 60)     | float32 | Pa\n",
      "T      | (1494, 60, 47) | float32 | K\n",
      "TROPPB | (1494, 60)     | float32 | Pa\n",
      "U      | (1494, 60, 47) | float32 | m s-1\n",
      "U10M   | (1494, 60)     | float32 | m s-1\n",
      "V      | (1494, 60, 47) | float32 | m s-1\n",
      "V10M   | (1494, 60)     | float32 | m s-1\n",
      "lat    | (1494, 60)     | float32 | degrees_north\n",
      "lev    | (47,)          | int16   | 1\n",
      "line   | (1494,)        | int16   | 1\n",
      "lon    | (1494, 60)     | float32 | degrees_east\n",
      "sample | (60,)          | int16   | 1\n",
      "time   | (1494,)        | float64 | seconds since 1993-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "h5fname = ('/mnt/sda1/data/OMI-Aura_ANC-OMVFPITMET'\n",
    "           '_2020m0216t225854-o82929_v003'\n",
    "           '-2020m0217t090311.nc4')\n",
    "\n",
    "data = h5py.File(h5fname, mode='r')\n",
    "\n",
    "for name, arr in data.items():\n",
    "    print(f\"{name:6s} | {str(arr.shape):14s} | \"\n",
    "          f\"{str(arr.dtype):7s} | {arr.attrs['units'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can lazily create a memory view into only part of one of the dataset arrays.  In the example, we have opened in read-only mode, but if we had opened using the `'r+'` or `'a'` modes we could change the file.  Use the `'w'` mode with extreme caution since it will overwrite an existing file.  If the mode allowed modification on disk, calling `data.flush()` or `data.close()` will write any changes back to the HDF5 source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let us create a view of only a small section of the 3-dimensional V dataset.  We are not particularly concerned here with understanding the domain of the data, but just demonstrating the APIs.  In particular, notice that we have used a stride in one dimension to show that the general NumPy style of complex memory views is available.  Only the data referenced is actually put into main memory, the rest stays on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[17.032158  , 12.763597  ,  3.7710803 ],\n",
       "        [16.53227   , 12.759642  ,  4.1722884 ]],\n",
       "\n",
       "       [[ 4.003829  , -1.0843939 , -6.7918572 ],\n",
       "        [ 3.818467  , -1.0030019 , -6.6708655 ]],\n",
       "\n",
       "       [[-2.7798688 ,  0.24923703, 20.513933  ],\n",
       "        [-2.690715  ,  0.2226392 , 20.473366  ]]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A 3-D block from middle of DELP array\n",
    "middle = data['V'][::500, 10:12, :3]\n",
    "middle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we modify the data in the view `middle`, it will be written back when we flush or close the handle (if not in read-only mode).  We might also use our data slice for other computations or data science purposes.  For example, perhaps such a selection acts as tensors input into a neural network.  In a simpler case, perhaps we simply want to find some statistics or reduction on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.782215  , 12.76162   ,  3.9716845 ],\n",
       "       [ 3.911148  , -1.0436978 , -6.7313614 ],\n",
       "       [-2.735292  ,  0.23593812, 20.493649  ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "middle.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creating a New HDF5 File\n",
    "\n",
    "Creating arrays within an HDF5 initially only allocates the disk sectors where they will live.  Filling them with data is a separate step.  This is similar to NumPy's `np.empty()` constructor that may allow random values to occur before an array is filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a brand new file (possibly delete an existing one)\n",
    "f = h5py.File('data/hierarchy.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# A 4-D array of integers\n",
    "f.create_dataset('/deeply/nested/group/my_data',\n",
    "                 (10, 10, 10, 10), dtype='i')\n",
    "# A 1-D array of integers\n",
    "f.create_dataset('deeply/path/elsewhere/other', (20,), dtype='i')\n",
    "# A 2-D array of floating point numbers\n",
    "f.create_dataset('deeply/path/that_data', (5, 5), dtype='f');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Assigning a group to a variable\n",
    "dset = f['/deeply/nested/group/my_data']\n",
    "# Fill the array with random values\n",
    "dset[...] = np.random.randint(0, 99, (10, 10, 10, 10))\n",
    "# Add some attributes to this dataset\n",
    "dset.attrs['author'] = 'David Mertz'\n",
    "dset.attrs['citation'] = 'INE Training'\n",
    "dset.attrs['shape_type'] = '4-D integer array'\n",
    "# Close file handle and write pending data\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reading Back Our New Serialization\n",
    "\n",
    "Having serialized several datasets (arrays) to a new HDF5 file, we can open it in `'r+'` mode that will allow modification, but will not overwrite the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10, 10, 10) int32\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('data/hierarchy.h5', 'r+')\n",
    "# Assign a nested dataset to a variable\n",
    "dset = f['/deeply/nested/group/my_data']\n",
    "print(dset.shape, dset.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author ðŸ † David Mertz\n",
      "citation ðŸ † INE Training\n",
      "shape_type ðŸ † 4-D integer array\n",
      "\n",
      "Data block:\n",
      " [[[[22 76]\n",
      "   [91 90]]]]\n"
     ]
    }
   ],
   "source": [
    "for key, val in dset.attrs.items():\n",
    "    print(key, \"ðŸ †\", val)\n",
    "print()\n",
    "print(\"Data block:\\n\", dset[5:6, 3:4, 2:4, 8:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can look at other groups and datasets as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elsewhere\n",
      "that_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"other\": shape (20,), type \"<i4\">"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A different dataset in nested groups\n",
    "for group in f['deeply/path']:\n",
    "    print(group)\n",
    "f['deeply/path/elsewhere/other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or find all objects (groups and datasets) within the overall HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deeply',\n",
       " 'deeply/nested',\n",
       " 'deeply/nested/group',\n",
       " 'deeply/nested/group/my_data',\n",
       " 'deeply/path',\n",
       " 'deeply/path/elsewhere',\n",
       " 'deeply/path/elsewhere/other',\n",
       " 'deeply/path/that_data']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "everything = []\n",
    "f.visit(everything.append)\n",
    "everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Modifying Existing Dataset\n",
    "\n",
    "We see that we have a 4-dimensional array of integer data.  Perhaps some metadata description was attached to it as well.  Let us also viewâ€”and then modifyâ€”some section of the data.  After we change the data, we can write it back to disk.  We can similarly change or add attribues in a regular dictionary style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dset.attrs['author'] = \"David Q Mertz\"\n",
    "dset.attrs['negative_elements'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us modify the same slice of data we displayed, then close the file handle to write it back to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data block:\n",
      " [[[[-69 -32]\n",
      "   [-20 -60]]]]\n"
     ]
    }
   ],
   "source": [
    "dset[5:6, 3:4, 2:4, 8:] = np.random.randint(-99, 0, (1, 1, 2, 2))\n",
    "print(\"Data block:\\n\", dset[5:6, 3:4, 2:4, 8:])\n",
    "# write change to disk\n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looking at our changes to verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Superset data block:\n",
      " [[[[ 60  67  97]\n",
      "   [ 59 -69 -32]\n",
      "   [ 84 -20 -60]\n",
      "   [ 36  31  27]]]]\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('data/hierarchy.h5', 'r+')\n",
    "dset = f['/deeply/nested/group/my_data']\n",
    "print(\"Superset data block:\\n\", dset[5:6, 3:4, 1:5, 7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author ðŸ † David Q Mertz\n",
      "citation ðŸ † INE Training\n",
      "negative_elements ðŸ † True\n",
      "shape_type ðŸ † 4-D integer array\n"
     ]
    }
   ],
   "source": [
    "for key, val in dset.attrs.items():\n",
    "    print(key, \"ðŸ †\", val)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
